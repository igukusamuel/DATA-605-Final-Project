---
title: "DATA 605 Final Exam"
author: "Samuel I Kigamba"
date: "May 20, 2020"
output:
  html_document: slidy_presentation
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load all the necessary Libraries

```{r, message=FALSE, warning=FALSE}

library(gridExtra)
library(RColorBrewer)
library(Matrix)
library(scales)
library(corrplot)
library(MASS)
library(psych)
library(ggplot2)

```

Your final is due by the end of the last week of class (5/22/2020).
You should post your solutions to your GitHub account or RPubs.  
You are also expected to make a short presentation via YouTube  and post that recording to the board.
This project will show off your ability to understand the elements of the class. 


## Problem 1

Using R, generate a random variable X that has 10,000 random uniform numbers from 1 to N, where N can be any number of your choosing greater than or equal to 6.  Then generate a random variable Y that has 10,000 random normal numbers with a mean of:

\[\mu=\sigma=(N+1)/2\]  

```{r}

set.seed(100)

N = 10
n = 10000

# Random Uniform number generation
X = runif(n, 1, N)

# Random normal number generation
Y = rnorm(n, mean = (N+1)/2, sd = (N+1)/2)

dfXY = data.frame(cbind(X, Y))

# View the first 5 numbers generated
dfXY[0:5,1:2]

```

--------------------------------------------------------------------------------

\clearpage


### Probability

Assume the small letter "x" is estimated as the median of the X variable, and the small letter "y" is estimated as the 1st quartile of the Y variable.  

```{r}
# Median of X
x = median(X)
x
```

```{r}
# 1st Quantile of Y
y = quantile(Y, 0.25)
y

```

--------------------------------------------------------------------------------

\clearpage

Calculate as a minimum the below probabilities a through c. Interpret the meaning of all probabilities.

#### a.  P(X>x | X>y)	

```{r}
# Number of observations where X is greater than y, the 1st quantile of Y
nXy = nrow(subset(dfXY, X > y))
nXy

# Probability of X is higher than x, the median of X given that X is greater than y, the 1st quantile of Y.
pa = nrow(subset(dfXY, X > x & X > y))/nXy
pa

```
The probability is 0.54 or 54%

#### b.  P(X>x, Y>y)		

```{r}
# Probability that both X and Y are greater than x, the median of X and y, the first quantile of Y respectively.
pb = nrow(subset(dfXY, X > x & Y > y))/n
pb

```
The probability is 0.3755 or 37.55%

#### c.  P(X<x | X>y)

```{r}
# Probability that X is less that x, the median of X given that X is greater than Y
pc = nrow(subset(dfXY, X < x & X > y))/nXy
pc

```

The probability is 0.4579 or 45.79%

--------------------------------------------------------------------------------

\clearpage

Investigate whether P(X>x and Y>y) = P(X > x) * P(Y > y) by building a table and evaluating the marginal and joint probabilities.

```{r}

A1 <- c(sum(X <= x & Y <= y), sum(X > x & Y <= y))
B1 <- c(sum(X <= x & Y > y), sum(X > x & Y > y))

ct_matrix <- matrix(c(A1, B1), nrow = 2)
ct_matrix <- rbind(ct_matrix, apply(ct_matrix, 2, sum))
ct_matrix <- cbind(ct_matrix, apply(ct_matrix, 1, sum))

xy <- c("<=1st quartile", ">1st quartile", "Total")
countDF <- data.frame(xy, ct_matrix)
colnames(countDF) <- c("x/y", "<=1st quartile", ">1st quartile", "Total")
print(countDF)

```


```{r}

A <- countDF[2, 4]
B <- countDF[3, 3]
A_B <- countDF[2, 3]
tot <- countDF[3, 4]

Prob_A <- A/tot
Prob_B <- B/tot
prob_A_B <- A_B/tot

print(prob_A_B)

```
So P(AB) = 0.3755

```{r}

Prob_A_Prob_B = Prob_A * Prob_B

print(Prob_A_Prob_B)

```

So, P(X>x and Y>y)=P(X>x)P(Y>y) is TRUE.

--------------------------------------------------------------------------------

\clearpage

Check to see if independence holds by using Fisher's Exact Test and the Chi Square Test.  
What is the difference between the two? 
Which is most appropriate?

```{r}
# Generate the matrix
Matrix <- matrix(c(A1, B1), nrow = 2)

# Perform the Chi square test
chisq.test(Matrix)
```

```{r}
# Perform the Fisher's Exact Test
fisher.test(Matrix, simulate.p.value=TRUE)
```


## Problem 2

You are to register for Kaggle.com (free) and compete in the House Prices: Advanced Regression Techniques competition.  https://www.kaggle.com/c/house-prices-advanced-regression-techniques .  
I want you to do the following.

```{r}
#Training set
train = read.csv("https://raw.githubusercontent.com/igukusamuel/DATA-605-Final-Project/master/train.csv")
dim(train)

# Select the variables to perform the various statistical operations
X = train$LotArea
Y = train$SalePrice
Z = train$GarageArea
```

--------------------------------------------------------------------------------

\clearpage

### Descriptive and Inferential Statistics.

Provide univariate descriptive statistics and appropriate plots for the training data set.  

Descriptive statistics on LotArea

```{r}
# Descriptive statistics on LotArea
summary(X)
describe(X)

```

*There are 1460 obsevarions with LotArea ranging from 1300SF to 215,245SF. The average LotArea is 10,516SF.*

```{r}
# BoxPlot and Histograms for the LotArea
par(mfrow=c(1,2))
boxplot(X, main="LotArea BoxPlot")
hist(X, breaks = 20, main = "LotArea Histogram")
```

*The distribution of the LotArea is right skewed with a alot of outliers.*

--------------------------------------------------------------------------------

\clearpage

Descriptive statistics on SalePrice

```{r}
# Descriptive statistics on SalePrice
summary(Y)
describe(Y)
```

*There are 1460 obsevarions with SalePrice ranging from $34,900 to $755,000. The average SalePrice is $180,921.*

```{r}
# BoxPlot and Histograms for the SalePrice
par(mfrow=c(1,2))
boxplot(Y, main="SalePrice BoxPlot")
hist(Y, breaks = 30, main = "SalePrice Histogram")

```

*The distribution of the SalePrice is right skewed with a couple of outliers.*

--------------------------------------------------------------------------------

\clearpage

Descriptive statistics on GarageArea

```{r}
# Descriptive statistics on GarageArea
summary(Z)
describe(Z)
```

*There are 1460 obsevarions with GarageArea ranging from 0SF to 1,418SF. The average GarageArea is 473SF.*

BoxPlot and Histograms for the GarageArea

```{r}
# BoxPlot and Histograms for the GarageArea
par(mfrow=c(1,2))
boxplot(Z, main="GarageArea BoxPlot")
hist(Z, breaks = 30, main = "GarageArea Histogram")
```

*The distribution of the GarageArea is right skewed with a few observable outliers.*

--------------------------------------------------------------------------------

\clearpage

Provide a scatterplot matrix for at least two of the independent variables and the dependent variable. 

```{r}
# Scatterplot of LotArea and GarageArea vs SalePrice
par(mfrow=c(1,2))
plot(X, Y, xlab="LotArea", ylab="SalePrice", main="LotArea vs SalePrice")
plot(Z, Y, xlab="GarageArea", ylab="SalePrice", main="GarageArea vs SalePrice")
```

*From the observation there seem to be no correlation between the LotArea and the SalePrice but there seem to be some form of correlation between*
*the GarageArea and the SalePrice. This is not surprising since a large garage would most likely be associated with a large house which is poised to* *directly affect the saleprice.*

--------------------------------------------------------------------------------

\clearpage

Derive a correlation matrix for any three quantitative variables in the dataset.  

```{r}

corDF = train[c("LotArea", "GarageArea", "SalePrice")]
corMatrix = cor(corDF, use = "complete.obs")
print(corMatrix)

#visualize the correlation matrix
corrplot(corMatrix, method = "circle")
```

*From the corellation matrix we can conclude that there exist strong to weak correlation between the three variables*
*SalePrice has a strong corellation with GarageArea and a weak corellation with LotArea with corellation coefficient of 0.62 and 0.26 respectively.*
*LotArea and GarageArea have weak corellation with each other with a correlation coefficieant of 0.18.*

--------------------------------------------------------------------------------

\clearpage

Test the hypotheses that the correlations between each pairwise set of variables is 0 and provide an 80% confidence interval.  

```{r}
# Pairwise corelation between LotArea and SalePrice
cor.test(corDF$LotArea, corDF$SalePrice, method = 'pearson', conf.level = 0.80)
```

```{r}
# Pairwise corelation between GarageArea and SalePrice
cor.test(corDF$GarageArea, corDF$SalePrice, method = 'pearson', conf.level = 0.80)
```

```{r}
# Pairwise corelation between LotArea and GarageArea
cor.test(corDF$LotArea, corDF$GarageArea, method = 'pearson', conf.level = 0.80)

```

Discuss the meaning of your analysis.

*The tests for pairwise corelation using pearson method estimated the association between the paired samples and computed a test of the value*
*being zero. All the p-value are less than the significant level alpha = 0.08 and we thus conclude that the pairwise variables are correlated with *
*respective correlation coefficient shown.*

Would you be worried about familywise error? Why or why not?

*Due to the fact that there are many variables in the train dataset that might have a huge impact on the correlation of the peirwise variables*
*Yes i would be worried. Its possible to reject TRUE NULL Hypothesis unless all other variables are considered.*

--------------------------------------------------------------------------------

\clearpage

 
### Linear Algebra and Correlation.

Invert your correlation matrix from above. 
This is known as the precision matrix and contains variance inflation factors on the diagonal.

```{r}
# Co-variance martix from above
print(corMatrix)

#To invert the Corelation Matrix use the solve() function
precision_matrix = solve(corMatrix)
print(precision_matrix)
```

--------------------------------------------------------------------------------

\clearpage

Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix.

```{r}
# To multiply the correlation matrix by the precision matrix
round((corMatrix %*% precision_matrix), 4)

# Multiply precision matrix by the correlation matrix
round((precision_matrix %*% corMatrix), 4)
```

*Both of the above operations produce an identity matrix.*

--------------------------------------------------------------------------------

\clearpage

Conduct LU decomposition on the matrix.

Correlation Matrix

```{r}
# LU decomposition of the correlation matrix
lud_cor = lu(corMatrix)
elu_cor = expand(lud_cor)

#Lower triangular matrix
cor_L = elu_cor$L
print(cor_L)

#Upper triangular matrix
cor_U = elu_cor$U
print(cor_U)
```

--------------------------------------------------------------------------------

\clearpage

Precision Matrix

```{r}
# LU decomposition of the precision matrix
lud_prec = lu(precision_matrix)
elu_prec = expand(lud_prec)

#Lower triangular matrix
prec_L = elu_prec$L
print(prec_L)

#Upper triangular matrix
prec_U = elu_prec$U
print(prec_U)
```

--------------------------------------------------------------------------------

\clearpage

Multiply the Upper triangulat matrix with the lower triangular matrix

```{r}
# Multiply cor_U with cor_L to get the original matrix since A = LU
cor_L %*% cor_U

# Ditto prec_U with prec_L
prec_L %*% prec_U

```

*Multiplication of L & U returns their respective origina matrix as expected.*

--------------------------------------------------------------------------------

\clearpage

  
### Calculus-Based Probability & Statistics.

Many times, it makes sense to fit a closed form distribution to data.  Select a variable in the Kaggle.com training dataset that is skewed to the right, shift it so that the minimum value is absolutely above zero if necessary.

```{r}
# We will use the GarageAre data. Check minimum and see if shifting is necessary.
z = corDF$GarageArea
min(z)

hist(z, breaks = 50)
```

--------------------------------------------------------------------------------

\clearpage


Then load the MASS package and run fitdistr to fit an exponential probability density function.  
(See  https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html ).  

```{r}
# Fit an exponential probability density function
fit_exp = fitdistr(z, densfun = "exponential")
print(fit_exp)
```

Find the optimal value of lambda for this distribution, and then take 1000 samples from this exponential distribution using this value (e.g., rexp(1000, lambda)). 

```{r}
# Get 1000 samples from the exponential distribution
fit_exp$estimate
samp = rexp(1000, fit_exp$estimate)
```

--------------------------------------------------------------------------------

\clearpage

Plot a histogram and compare it with a histogram of your original variable.   

```{r}
# Plot a histogram of the observed and the simulated data
par(mfrow=c(1,2))
hist(z, breaks = 100, xlab = "Observed_GarageArea", main = "Observed")
hist(samp, breaks = 100, xlab = "Simulated_GarageArea", main = "Simulated")
```

*Visually from the histograms the simulated data is more heavily skewed to the right while the observed data is more concentrated to the centre.*

--------------------------------------------------------------------------------

\clearpage

Using the exponential pdf, find the 5th and 95th percentiles using the cumulative distribution function (CDF).   

```{r}
# Find the 5th and 95th percentile of the simulated sample data
quantile(samp, probs = c(0.05, 0.95))
```

Also generate a 95% confidence interval from the empirical data, assuming normality.  

```{r}
# Calculating the 95% confidence interval on the observed data
mean_z = mean(z)
n_z = nrow(z)
sd_z = sd(z)

se = qnorm(0.975) * sd_z/sqrt(n)

left_int = mean_z - se
print(left_int)

right_int = mean_z + se
print(right_int)

```

*The 95% confidence interval lies between 468.79 and 477.17.*

--------------------------------------------------------------------------------

\clearpage

```{r}
# We plot a histogram to show the assumed normality
assume_normality = rnorm(length(z), mean_z, sd_z)
hist(assume_normality)
```


Finally, provide the empirical 5th percentile and 95th percentile of the data.  Discuss.

```{r}
# Emperical 5th and 95th percentile of the observed data
quantile(z, probs = c(0.05, 0.95))
```

## Modeling.  

Build some type of multiple regression  model and submit your model to the competition board.  
Provide your complete model summary and results with analysis.

### Training Data and Model Generation

```{r}

#Load Training data set into a data frame and review the summary statistics for missing and or limited features.
hd_tr = read.csv("https://raw.githubusercontent.com/igukusamuel/DATA-605-Final-Project/master/train.csv")

summary(hd_tr)
```
--------------------------------------------------------------------------------

\clearpage

Load the selected variables into a dataframe and perform data cleanup operations.

```{r}
# Remove features with missing or limited data as evaluated from the summary statistics above.
hd_train <- hd_tr[, c("MSSubClass", "MSZoning", "LotArea", "LotShape", "LotConfig", "Neighborhood", "Condition1", 
                      "Street", "BldgType", "HouseStyle", "OverallQual", "OverallCond", "YearBuilt", "YearRemodAdd",
                      "RoofStyle", "Exterior2nd", "MasVnrType", "ExterQual", "BsmtQual", "BsmtCond", "BsmtExposure",
                      "BsmtFinType1", "BsmtFinSF1", "BsmtFinType2", "TotalBsmtSF", "HeatingQC", "Electrical", 
                      "X1stFlrSF", "GrLivArea", "BsmtFullBath", "BsmtHalfBath", "FullBath", "HalfBath", "BedroomAbvGr",
                      "KitchenAbvGr", "KitchenQual", "TotRmsAbvGrd", "Functional", "GarageArea", "PavedDrive", 
                      "WoodDeckSF", "OpenPorchSF", "YrSold", "SaleType", "SaleCondition", "SalePrice")]


# Remove all Na's from the training data set
hd_train = na.omit(hd_train)

# Generate an initial regression model
Model_1 = lm(SalePrice ~ ., data = hd_train)

# Generate Summary Statisics
summary(Model_1)

```

The model produces a Multiple R-Squared of 0.8833 which is great. We can interpret this to mean that 88.33% variance in the saleprice van be explained by the predictor variables in the current model.
The F-Statistic is 62.26 on 153 and 1258 degrees of freedom. The p-value is very small.

--------------------------------------------------------------------------------

\clearpage

### Model Testing and submission

Let us test how good our model performs on the test data set.

```{r}
# Load Testing set
hd_test = read.csv("https://raw.githubusercontent.com/igukusamuel/DATA-605-Final-Project/master/test.csv")

# Predict the saleprice for the testing data set
PredictedDataModel = hd_test
PredictedDataModel$SalePrice = predict(Model_1, hd_test)

# Set the Id and SalePrice colums to be loaded into Kaggle Data Frame
Id = hd_test$Id
SalePrice = PredictedDataModel$SalePrice

# Combine the ID and the model predicted SalePrice values
DF = data.frame(cbind(Id, SalePrice))

# Replace all missing values with 0 in the final Data Frame
DF[is.na(DF)] = 0
DF = replace(DF, is.na(DF), 0)

#View the first 5 rows of the data frame
print(head(DF, 5))

# Write the predicted values into a .csv file for submission.
#write.csv(DF, file = "KaggleData_Model_Prediction.csv", row.names = FALSE)

```

--------------------------------------------------------------------------------

\clearpage

### Kaggle Submission - Score

Report your Kaggle.com user name and score.

Kaggle Username: Samuel Kigamba

Kaggle Score: 2.50090

See submission documents : https://github.com/igukusamuel/DATA-605-Final-Project

![](https://github.com/igukusamuel/DATA-605-Final-Project/blob/master/Kaggle_Submission.PNG)



